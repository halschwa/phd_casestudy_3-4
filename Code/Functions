### helper functions

# get a list of papers ready to be analysed
getPeriod <- function(viewName)
{
	papers <- dbGetQuery(conn, paste("select * from",viewName))
	# the corpus tools need a 2 unique columns: doc_id and text
	# use id as doc_id
	papers$doc_id <- papers$id
	papers$id <- NULL
	# use abstract as text
	papers$text <- papers$abstract
	papers$abstract <- NULL
	return(papers)
}

#process and build the corpus
createCorpus <- function( texts )
{
  # final list is custom words + standard english words
  stopWords <- tm::stopwords("en")
  stopWords <- c(stopWords, wordsToIgnore)
  
  corpus <- Corpus(DataframeSource(texts))
  corpus <- tm_map(corpus, content_transformer(tolower))
  removeSpecialChars <- function(x) gsub("[^a-zA-Z0-9 ]","",x)
  corpus <- tm_map(corpus, removeSpecialChars)
  corpus <- tm_map(corpus, removeNumbers)
  #remove stem - because the full word was not presented in results; irritating me
  #corpus<- tm_map(corpus, stemDocument, language = "en")
  corpus<- tm_map(corpus, removeWords, stopWords)
  # remove dashes after deletion of words with dashes (some times dashes were still present)
  #	corpus <- tm_map(corpus, removePunctuation, preserve_intra_word_dashes = FALSE)
  corpus <- tm_map(corpus, stripWhitespace)
  return(corpus)
}

# Document-Term Matrix based on corpus and minimum frequency
createDTM <- function( corpus )
{
  DTM <- DocumentTermMatrix(corpus, control = list(bounds = list(global = c(minimumFrequency, Inf))))
  return(DTM)
}

# functions to remove any document without text in the odd case that they are empty once all terms appearing less than minimumFrequency are removed
getPresentIdx <- function(DTM)
{
  presentIdx <- slam::row_sums(DTM) > 0
  return(presentIdx)
}
removeEmptyRows <- function( presentIdx, dataset )
{
  # because some documents do not have any term repeated in other documents we have empty rows (e.g. rows with no terms)
  # LDA will not like this so we remove them from the analysis
  return(dataset[presentIdx,])
}

# compute and plot indexes to choose topic modelling number of topics
exploreNumberTopics <- function( dtm )
{
	resultsTuning <- ldatuning::FindTopicsNumber(dtm, topics = seq(from = 4, to = 19, by = 1), metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"), method = "Gibbs", verbose = TRUE)
  	g1 <- ggplot(resultsTuning, aes(x=topics, y=CaoJuan2009)) + geom_line() + geom_point() + ggtitle("minimize CaoJuan2009") + theme_bw() + scale_x_continuous(breaks=seq(min(resultsTuning$topics), max(resultsTuning$topics,1)))
	  g2 <- ggplot(resultsTuning, aes(x=topics, y=Arun2010)) + geom_line() + geom_point() + ggtitle("minimize Arun 2010") + theme_bw() + scale_x_continuous(breaks=seq(min(resultsTuning$topics), max(resultsTuning$topics,1)))
  
	  g3 <- ggplot(resultsTuning, aes(x=topics, y=Deveaud2014)) + geom_line() + geom_point() + ggtitle("maximize Deveaud2014") + theme_bw()+ scale_x_continuous(breaks=seq(min(resultsTuning$topics), max(resultsTuning$topics,1)))
	  g4 <- ggplot(resultsTuning, aes(x=topics, y=Griffiths2004)) + geom_line() + geom_point() + ggtitle("maximize Griffiths2004") + theme_bw()+ scale_x_continuous(breaks=seq(min(resultsTuning$topics), max(resultsTuning$topics,1)))
	  return(grid.arrange(g1, g2, g3, g4, ncol=2))
}

# get list of top terms per topic after LDA
getTopTerms <- function(lda, numTermsPerTopic)
{
	betas <- tidy(lda, matrix ="beta") %>% arrange(desc(beta)) %>% group_by(topic) %>% slice(1:numTermsPerTopic)
	betas$topic <- paste("Topic n.",as.character(betas$topic))
	return(betas)
}


# visualize as an ordered barplot the top terms of each topic
plotTopTerms <- function(topTerms)
{
ggplot(topTerms, aes(x=beta, y=reorder_within(term, beta, topic))) + geom_bar(stat="identity") + facet_wrap(~topic, scales="free_y", strip.position = "top") + theme(legend.position="none") + theme_bw() + ggtitle("Top Terms per Topic") + scale_y_reordered()+ ylab("top terms") + xlab("term weight for topic")
}


